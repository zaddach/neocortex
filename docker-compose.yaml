# https://gist.githubusercontent.com/usrbinkat/de44facc683f954bf0cca6c87e2f9f88/raw/0402e8441de57ccd8b00fe0db8ad40cae7d5fdb8/docker-compose.yaml
services:
  open-webui:
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:main
    environment:
      - MODEL_DOWNLOAD_DIR=/models
      - OLLAMA_API_BASE_URL=http://ollama:11434
      - OLLAMA_API_URL=http://ollama:11434
      - LOG_LEVEL=debug
      - WEBUI_SECRET_KEY=your_secret_key_here  # Add this to prevent logouts after updates
      - DOCLING_SERVER_URL=http://docling:5001
      - CONTENT_EXTRACTION_ENGINE=docling
      - 'DOCLING_PARAMS={
          "do_ocr": true,
          "ocr_engine": "tesseract",
          "ocr_lang": ["eng", "fra", "deu", "spa"],
          "force_ocr": false,
          "pdf_backend": "dlparse_v4",
          "table_mode": "accurate",
          "do_picture_description": true,
          "picture_description_mode": "api",
          "picture_description_api_model": "gpt-4o",
          "vlm_pipeline_model_api": "openai://gpt-4o"
        }'
      - RAG_EMBEDDING_ENGINE=ollama
      - RAG_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - RAG_TOP_K=3
      #- VECTOR_DB=pgvector
      #- PGVECTOR_DB_URL=postgres://postgres:example@postgres:5432/postgres
      - 'TOOLSERVER_CONNECTIONS=[
          {
            "type": "openapi",
            "url": "http://neo4j-mcp:80",
            "spec_type": "url",
            "spec": "",
            "path": "openapi.json",
            "auth-type": "none",
            "key": "",
            "config": {"enable": true},
            "info": {
              "id": "",
              "name": "neo4j"
              "description": "A tool to query the Neo4j graph database for relationships and insights."
            }
          },
          {
            "type": "openapi",
            "url": "http://pyghidra-mcp:80",
            "spec_type": "url",
            "spec": "",
            "path": "openapi.json",
            "auth-type": "none",
            "key": "",
            "config": {"enable": true},
            "info": {
              "id": "",
              "name": "ghidra"
              "description": "A tool to analyze and extract information from binary files using PyGhidra."
            }
          },
          {
            "type": "openapi",
            "url": "http://angr-mcp:80",
            "spec_type": "url",
            "spec": "",
            "path": "openapi.json",
            "auth-type": "none",
            "key": "",
            "config": {"enable": true},
            "info": {
              "id": "",
              "name": "angr"
              "description": "A tool to perform binary analysis and symbolic execution using the Angr framework."
            }
          },
          {
            "type": "openapi",
            "url": "http://triage-mcp:80",
            "spec_type": "url",
            "spec": "",
            "path": "openapi.json",
            "auth-type": "none",
            "key": "",
            "config": {"enable": true},
            "info": {
              "id": "",
              "name": "triage"
              "description": "A tool to analyze and classify malware samples using the Triage framework."
            }
          }
        ]'

    volumes:
      - data:/data
      - models:/models
      - open-webui:/app/backend/data  # Corrected path based on documentation
    ports:
      - "8080:8080"
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "2"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - ollama-net
    depends_on:
      neo4j-mcp:
        condition: service_healthy
      pyghidra-mcp:
        condition: service_healthy
      angr-mcp:
        condition: service_started
      ollama:
        condition: service_started
      triage-mcp:
        condition: service_started
      postgres:
        condition: service_started
    restart: unless-stopped

  postgres:
    image: postgres
    restart: always
    shm_size: '128mb'
    environment:
      POSTGRES_PASSWORD: example
    
  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - LOG_LEVEL=debug
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              count: all
    volumes:
      - ollama:/root/.ollama
      - models:/models
    ports:
      - "11434:11434"
    logging:
      driver: json-file
      options:
        max-size: "5m"
        max-file: "2"
    networks:
      - ollama-net
    restart: unless-stopped
    # healthcheck:
    #   test: ["CMD", "curl", "-Lf", "http://localhost:11434/"]
    #   interval: 30s
    #   timeout: 3s
    #   retries: 30
    
  watchtower:
    image: ghcr.io/nicholas-fedor/watchtower:latest
    container_name: watchtower
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: --interval 300 open-webui  # Check for updates every 5 minutes
    depends_on:
      - open-webui
    networks:
      - ollama-net
    restart: unless-stopped
  
  neo4j:
    image: neo4j
    container_name: neo4j
    environment:
      - "NEO4J_AUTH=neo4j/iesha8aeMohjahchah4v"
      - "NEO4J_apoc_export_file_enabled=true"
      - "NEO4J_apoc_import_file_enabled=true"
      - "NEO4J_apoc_import_file_use__neo4j__config=true"
      - "NEO4J_PLUGINS=[\"apoc\"]"
    ports:
      - "7474:7474"
      - "7687:7687"
    networks:
      - ollama-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-O", "-", "-q", "http://localhost:7474/"]
      interval: 30s
      timeout: 3s
      retries: 30

  neo4j-mcp:
    build: ./neo4j-mcp
    container_name: neo4j-mcp
    environment:
      - "NEO4J_URI=neo4j://neo4j:7687"
      - "NEO4J_USERNAME=neo4j"
      - "NEO4J_PASSWORD=iesha8aeMohjahchah4v"
      - "NEO4J_DATABASE=neo4j"
      - "NEO4J_READ_ONLY=false"
      - "NEO4J_TELEMETRY=true"
    ports:
      - "8004:80"
    networks:
      - ollama-net
    restart: unless-stopped
    depends_on:
      neo4j:
        condition: service_healthy

    healthcheck:
      test: ["CMD", "curl", "-Lf", "http://localhost/openapi.json"]
      interval: 30s
      timeout: 3s
      retries: 30

  pyghidra-mcp:
    build: ./pyghidra-mcp
    container_name: pyghidra-mcp
    ports:
      - "8003:80"
    networks:
      - ollama-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-Lf", "http://localhost/openapi.json"]
      interval: 30s
      timeout: 3s
      retries: 30

  angr-mcp:
    build: ./angr-mcp
    container_name: angr-mcp
    ports:
      - "8005:80"
    networks:
      - ollama-net
    restart: unless-stopped
    # healthcheck:
    #   test: ["CMD", "curl", "-Lf", "http://localhost/openapi.json"]
    #   interval: 30s
    #   timeout: 3s
    #   retries: 30

  triage-mcp:
    image: quay.io/docling-project/docling-serve-cu124
    



    build: ./triage-mcp
    container_name: triage-mcp
    ports:
      - "8007:80"
    networks:
      - ollama-net
    restart: unless-stopped
    # healthcheck:
    #   test: ["CMD", "curl", "-Lf", "http://localhost/openapi.json"]
    #   interval: 30s
    #   timeout: 3s
    #   retries: 30

  # static-malware-analyzer:
  #   build: https://github.com/grchanduvardhan/Static-Malware-PE-and-DLL-Analyzer
  #   container_name: static-malware-analyzer
  #   ports:
  #     - "8006:5000"
  #   networks:
  #     - ollama-net
  #   restart: unless-stopped


  docling:
    image: quay.io/docling-project/docling-serve-cu124
    container_name: docling
    ports:
      - "5001:5001"
    environment:
      - DOCLING_SERVE_ENABLE_UI=true
    networks:
      - ollama-net
    restart: unless-stopped
    deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                capabilities: [gpu]
                count: all

volumes:
  data:
  models:
  ollama:
  open-webui:
networks:
  ollama-net:
    driver: bridge
